forward 1rho0.10, first run no save model .....
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996710
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:41
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f63402b6790>
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(47.1357, grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 47.13569094872642
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 47.13569094872642
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 47.13569094872642
epoch  2  acc reg_loss 4.713569094872642
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 4.713569094872642
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 4.713569094872642
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 4.713569094872642
5 epoch: train_loss:0.679919 valid_loss:1.272061 each epoch time:0.14393
optimizer lr 0.00100  boxsize 12.64911  train_dq/boxsize 0.043408  valid_dq/boxsize 0.074951  train_dp 0.615170  valid_dp 0.610925 reg_loss 4.713569
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:42
run time  0:00:01.091796
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/rho0/forward_1/forward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/1rho0.10/forward_1/forward_1_loss.txt
forward 2rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996727
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:43
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f448cc04190>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 5, '_step_count': 6, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(196.7671, grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 196.7671070111364
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 196.7671070111364
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 196.7671070111364
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 196.7671070111364
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 196.7671070111364
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 196.7671070111364
epoch  1  acc reg_loss 19.67671070111364
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 19.67671070111364
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 19.67671070111364
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 19.67671070111364
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 19.67671070111364
5 epoch: train_loss:0.944698 valid_loss:1.374283 each epoch time:0.14964
optimizer lr 0.00100  boxsize 10.69045  train_dq/boxsize 0.070576  valid_dq/boxsize 0.093246  train_dp 0.612730  valid_dp 0.616917 reg_loss 19.676711
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:44
run time  0:00:01.120769
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/2rho0.14/forward_1/forward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/2rho0.14/forward_1/forward_1_loss.txt
forward 3rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996742
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:45
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fca31e64550>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 10, '_step_count': 11, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.813861 valid_loss:1.313824 each epoch time:0.15083
optimizer lr 0.00100  boxsize 12.64911  train_dq/boxsize 0.055929  valid_dq/boxsize 0.078977  train_dp 0.559795  valid_dp 0.562008 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:46
run time  0:00:01.158044
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/3rho0.10/forward_1/forward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/3rho0.10/forward_1/forward_1_loss.txt
forward 4rho0.20
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996757
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:46
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.2lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f4c7480de50>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 15, '_step_count': 16, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  8.94427190999916 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(7.4919, grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 7.491902004360355
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 7.491902004360355
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 7.491902004360355
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 7.491902004360355
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 7.491902004360355
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 7.491902004360355
epoch  1  acc reg_loss 0.7491902004360356
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.7491902004360356
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.7491902004360356
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.7491902004360356
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.7491902004360356
5 epoch: train_loss:1.107656 valid_loss:1.003015 each epoch time:0.15072
optimizer lr 0.00100  boxsize 8.94427  train_dq/boxsize 0.093144  valid_dq/boxsize 0.086328  train_dp 0.643113  valid_dp 0.637820 reg_loss 0.749190
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:47
run time  0:00:01.112993
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth ../data/gen_by_ML/pw-auto-test/4rho0.20/forward_1/forward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_loss.txt ../data/gen_by_ML/pw-auto-test/4rho0.20/forward_1/forward_1_loss.txt
forward 5rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996772
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:48
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fe00baaf090>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 20, '_step_count': 21, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.797890 valid_loss:0.995568 each epoch time:0.14821
optimizer lr 0.00100  boxsize 12.64911  train_dq/boxsize 0.055878  valid_dq/boxsize 0.066076  train_dp 0.546175  valid_dp 0.544975 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:49
run time  0:00:01.089834
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/5rho0.10/forward_1/forward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/5rho0.10/forward_1/forward_1_loss.txt
forward 6rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996787
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:50
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7eff78c0fcd0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 25, '_step_count': 26, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.833372 valid_loss:0.981357 each epoch time:0.14803
optimizer lr 0.00100  boxsize 10.69045  train_dq/boxsize 0.066191  valid_dq/boxsize 0.074945  train_dp 0.576761  valid_dp 0.582618 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:51
run time  0:00:01.107557
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/6rho0.14/forward_1/forward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/6rho0.14/forward_1/forward_1_loss.txt
forward 7rho0.27
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996802
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:52
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.27lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f9e7ba4cc90>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 30, '_step_count': 31, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  7.69800358919501 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:1.046005 valid_loss:1.019992 each epoch time:0.14553
optimizer lr 0.00100  boxsize 7.69800  train_dq/boxsize 0.102665  valid_dq/boxsize 0.099690  train_dp 0.649154  valid_dp 0.656558 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:53
run time  0:00:01.119582
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth ../data/gen_by_ML/pw-auto-test/7rho0.27/forward_1/forward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_loss.txt ../data/gen_by_ML/pw-auto-test/7rho0.27/forward_1/forward_1_loss.txt
forward 8rho0.38
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996817
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:54
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.38lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.38lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.38lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f24654c3390>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 35, '_step_count': 36, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  6.488856845230502 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.740114 valid_loss:1.108912 each epoch time:0.14600
optimizer lr 0.00100  boxsize 6.48886  train_dq/boxsize 0.078887  valid_dq/boxsize 0.116309  train_dp 0.691436  valid_dp 0.734385 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:55
run time  0:00:01.070686
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/8rho0.38/8rho0.38_5.pth ../data/gen_by_ML/pw-auto-test/8rho0.38/forward_1/forward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/8rho0.38/8rho0.38_loss.txt ../data/gen_by_ML/pw-auto-test/8rho0.38/forward_1/forward_1_loss.txt
backward 7rho0.27
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996833
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:55
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.27lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f7abcab3c50>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/8rho0.38/8rho0.38_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 40, '_step_count': 41, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  7.69800358919501 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:1.001185 valid_loss:0.974058 each epoch time:0.14863
optimizer lr 0.00100  boxsize 7.69800  train_dq/boxsize 0.099647  valid_dq/boxsize 0.096532  train_dp 0.642468  valid_dp 0.649504 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:57
run time  0:00:01.084622
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth ../data/gen_by_ML/pw-auto-test/7rho0.27/backward_1/backward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_loss.txt ../data/gen_by_ML/pw-auto-test/7rho0.27/backward_1/backward_1_loss.txt
backward 6rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996848
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:57
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fde755762d0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 45, '_step_count': 46, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.608848 valid_loss:0.756594 each epoch time:0.14670
optimizer lr 0.00100  boxsize 10.69045  train_dq/boxsize 0.050039  valid_dq/boxsize 0.061302  train_dp 0.568055  valid_dp 0.571939 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:49:58
run time  0:00:01.099168
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/6rho0.14/backward_1/backward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/6rho0.14/backward_1/backward_1_loss.txt
backward 5rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996863
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:49:59
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f79e570ce50>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 50, '_step_count': 51, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.678065 valid_loss:1.173204 each epoch time:0.14342
optimizer lr 0.00100  boxsize 12.64911  train_dq/boxsize 0.049966  valid_dq/boxsize 0.074865  train_dp 0.527831  valid_dp 0.525783 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:00
run time  0:00:01.093235
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/5rho0.10/backward_1/backward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/5rho0.10/backward_1/backward_1_loss.txt
backward 4rho0.20
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996878
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:01
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.2lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fb59b2a9250>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 55, '_step_count': 56, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  8.94427190999916 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.726093 valid_loss:0.677240 each epoch time:0.14911
optimizer lr 0.00098  boxsize 8.94427  train_dq/boxsize 0.065845  valid_dq/boxsize 0.061056  train_dp 0.615833  valid_dp 0.615640 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:02
run time  0:00:01.156174
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth ../data/gen_by_ML/pw-auto-test/4rho0.20/backward_1/backward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_loss.txt ../data/gen_by_ML/pw-auto-test/4rho0.20/backward_1/backward_1_loss.txt
backward 3rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996893
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:03
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f07b2c3d0d0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 60, '_step_count': 61, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.773851 valid_loss:1.168605 each epoch time:0.14744
optimizer lr 0.00098  boxsize 12.64911  train_dq/boxsize 0.055854  valid_dq/boxsize 0.074860  train_dp 0.524117  valid_dp 0.521500 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:04
run time  0:00:01.151606
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/3rho0.10/backward_1/backward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/3rho0.10/backward_1/backward_1_loss.txt
backward 2rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996908
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:05
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f1230d24350>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 65, '_step_count': 66, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0.0532, grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.05318062295319237
epoch  5  acc reg_loss 0.005318062295319237
5 epoch: train_loss:0.598595 valid_loss:0.826232 each epoch time:0.14803
optimizer lr 0.00098  boxsize 10.69045  train_dq/boxsize 0.050019  valid_dq/boxsize 0.066169  train_dp 0.559113  valid_dp 0.570830 reg_loss 0.005318
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:06
run time  0:00:01.131082
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/2rho0.14/backward_1/backward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/2rho0.14/backward_1/backward_1_loss.txt
backward 1rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996928
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:06
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f31b208efd0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 70, '_step_count': 71, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.771517 valid_loss:1.166133 each epoch time:0.14755
optimizer lr 0.00098  boxsize 12.64911  train_dq/boxsize 0.055848  valid_dq/boxsize 0.074853  train_dp 0.521997  valid_dp 0.519281 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:07
run time  0:00:01.111935
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/1rho0.10/backward_1/backward_1_5.pth
cp ../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/1rho0.10/backward_1/backward_1_loss.txt
forward 2rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996943
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:08
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f449ce12dd0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 75, '_step_count': 76, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.524253 valid_loss:0.820241 each epoch time:0.17786
optimizer lr 0.00098  boxsize 10.69045  train_dq/boxsize 0.043344  valid_dq/boxsize 0.066128  train_dp 0.556371  valid_dp 0.566113 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:09
run time  0:00:01.252199
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/2rho0.14/forward_2/forward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/2rho0.14/forward_2/forward_2_loss.txt
forward 3rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996958
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:10
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fbabc870bd0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 80, '_step_count': 81, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.768070 valid_loss:1.161777 each epoch time:0.14885
optimizer lr 0.00098  boxsize 12.64911  train_dq/boxsize 0.055844  valid_dq/boxsize 0.074852  train_dp 0.518747  valid_dp 0.515107 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:11
run time  0:00:01.107518
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/3rho0.10/forward_2/forward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/3rho0.10/forward_2/forward_2_loss.txt
forward 4rho0.20
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996973
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:12
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.2lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f8fa0477f50>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 85, '_step_count': 86, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  8.94427190999916 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(18.9464, grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 18.94639652304855
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 18.94639652304855
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 18.94639652304855
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 18.94639652304855
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 18.94639652304855
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 18.94639652304855
epoch  1  acc reg_loss 1.8946396523048548
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 1.8946396523048548
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 1.8946396523048548
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 1.8946396523048548
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 1.8946396523048548
5 epoch: train_loss:0.870524 valid_loss:0.771240 each epoch time:0.14927
optimizer lr 0.00098  boxsize 8.94427  train_dq/boxsize 0.078908  valid_dq/boxsize 0.070558  train_dp 0.610254  valid_dp 0.610709 reg_loss 1.894640
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:13
run time  0:00:01.148868
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth ../data/gen_by_ML/pw-auto-test/4rho0.20/forward_2/forward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_loss.txt ../data/gen_by_ML/pw-auto-test/4rho0.20/forward_2/forward_2_loss.txt
forward 5rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3996988
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:14
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fd559dff750>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 90, '_step_count': 91, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.767964 valid_loss:1.061668 each epoch time:0.14804
optimizer lr 0.00098  boxsize 12.64911  train_dq/boxsize 0.055854  valid_dq/boxsize 0.070573  train_dp 0.518468  valid_dp 0.514571 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:15
run time  0:00:01.109363
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/5rho0.10/forward_2/forward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/5rho0.10/forward_2/forward_2_loss.txt
forward 6rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997003
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:16
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f12b10a1290>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 95, '_step_count': 96, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.520622 valid_loss:0.815990 each epoch time:0.14740
optimizer lr 0.00098  boxsize 10.69045  train_dq/boxsize 0.043347  valid_dq/boxsize 0.066165  train_dp 0.553064  valid_dp 0.561844 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:17
run time  0:00:01.105786
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/6rho0.14/forward_2/forward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/6rho0.14/forward_2/forward_2_loss.txt
forward 7rho0.27
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997018
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:17
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.27lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f9cae6a1350>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 100, '_step_count': 101, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  7.69800358919501 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.803919 valid_loss:0.851933 each epoch time:0.15004
optimizer lr 0.00098  boxsize 7.69800  train_dq/boxsize 0.082755  valid_dq/boxsize 0.086390  train_dp 0.630943  valid_dp 0.640051 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:19
run time  0:00:01.134385
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth ../data/gen_by_ML/pw-auto-test/7rho0.27/forward_2/forward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_loss.txt ../data/gen_by_ML/pw-auto-test/7rho0.27/forward_2/forward_2_loss.txt
forward 8rho0.38
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997033
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:19
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.38lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.38lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.38lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f4f0b3cde50>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 105, '_step_count': 106, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  6.488856845230502 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.670077 valid_loss:0.882402 each epoch time:0.14419
optimizer lr 0.00098  boxsize 6.48886  train_dq/boxsize 0.070559  valid_dq/boxsize 0.092930  train_dp 0.678568  valid_dp 0.720265 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:20
run time  0:00:01.088888
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/8rho0.38/8rho0.38_5.pth ../data/gen_by_ML/pw-auto-test/8rho0.38/forward_2/forward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/8rho0.38/8rho0.38_loss.txt ../data/gen_by_ML/pw-auto-test/8rho0.38/forward_2/forward_2_loss.txt
backward 7rho0.27
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997049
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:21
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.27lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f062f50bd10>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/8rho0.38/8rho0.38_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 110, '_step_count': 111, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  7.69800358919501 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.764371 valid_loss:0.849034 each epoch time:0.14647
optimizer lr 0.00098  boxsize 7.69800  train_dq/boxsize 0.078912  valid_dq/boxsize 0.086385  train_dp 0.628777  valid_dp 0.637823 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:22
run time  0:00:01.097837
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth ../data/gen_by_ML/pw-auto-test/7rho0.27/backward_2/backward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_loss.txt ../data/gen_by_ML/pw-auto-test/7rho0.27/backward_2/backward_2_loss.txt
backward 6rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997064
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:23
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f84119db950>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.00098, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 115, '_step_count': 116, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00098]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.517932 valid_loss:0.670185 each epoch time:0.14916
optimizer lr 0.00096  boxsize 10.69045  train_dq/boxsize 0.043330  valid_dq/boxsize 0.055970  train_dp 0.550787  valid_dp 0.558719 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:24
run time  0:00:01.102135
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/6rho0.14/backward_2/backward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/6rho0.14/backward_2/backward_2_loss.txt
backward 5rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997116
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:25
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f7fee80d690>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 120, '_step_count': 121, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.762438 valid_loss:1.055072 each epoch time:0.14714
optimizer lr 0.00096  boxsize 12.64911  train_dq/boxsize 0.055857  valid_dq/boxsize 0.070580  train_dp 0.513070  valid_dp 0.507969 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:26
run time  0:00:01.085560
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/5rho0.10/backward_2/backward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/5rho0.10/backward_2/backward_2_loss.txt
backward 4rho0.20
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997131
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:26
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.2lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f2b39fe53d0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 125, '_step_count': 126, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  8.94427190999916 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.710736 valid_loss:0.764259 each epoch time:0.14530
optimizer lr 0.00096  boxsize 8.94427  train_dq/boxsize 0.065881  valid_dq/boxsize 0.070551  train_dp 0.602922  valid_dp 0.605027 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:27
run time  0:00:01.101159
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth ../data/gen_by_ML/pw-auto-test/4rho0.20/backward_2/backward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_loss.txt ../data/gen_by_ML/pw-auto-test/4rho0.20/backward_2/backward_2_loss.txt
backward 3rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997146
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:28
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f74b8d14e50>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 130, '_step_count': 131, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.760719 valid_loss:1.053442 each epoch time:0.14436
optimizer lr 0.00096  boxsize 12.64911  train_dq/boxsize 0.055853  valid_dq/boxsize 0.070583  train_dp 0.511456  valid_dp 0.506290 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:29
run time  0:00:01.104474
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/3rho0.10/backward_2/backward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/3rho0.10/backward_2/backward_2_loss.txt
backward 2rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997161
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:30
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f43e6761190>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 135, '_step_count': 136, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.513472 valid_loss:0.596064 each epoch time:0.14800
optimizer lr 0.00096  boxsize 10.69045  train_dq/boxsize 0.043320  valid_dq/boxsize 0.050070  train_dp 0.546811  valid_dp 0.556369 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:31
run time  0:00:01.116491
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/2rho0.14/backward_2/backward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/2rho0.14/backward_2/backward_2_loss.txt
backward 1rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997176
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:32
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fa3b7baae50>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 140, '_step_count': 141, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.759262 valid_loss:0.952012 each epoch time:0.14993
optimizer lr 0.00096  boxsize 12.64911  train_dq/boxsize 0.055852  valid_dq/boxsize 0.066021  train_dp 0.510054  valid_dp 0.504580 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:33
run time  0:00:01.089424
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/1rho0.10/backward_2/backward_2_5.pth
cp ../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/1rho0.10/backward_2/backward_2_loss.txt
forward 2rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997191
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:34
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fe30359c7d0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 145, '_step_count': 146, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.511821 valid_loss:0.666774 each epoch time:0.14604
optimizer lr 0.00096  boxsize 10.69045  train_dq/boxsize 0.043316  valid_dq/boxsize 0.055929  train_dp 0.545336  valid_dp 0.556131 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:35
run time  0:00:01.145842
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/2rho0.14/forward_3/forward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/2rho0.14/forward_3/forward_3_loss.txt
forward 3rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997206
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:36
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f0b063b3650>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 150, '_step_count': 151, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.658117 valid_loss:1.050474 each epoch time:0.14880
optimizer lr 0.00096  boxsize 12.64911  train_dq/boxsize 0.049955  valid_dq/boxsize 0.070586  train_dp 0.508764  valid_dp 0.503291 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:37
run time  0:00:01.094321
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/3rho0.10/forward_3/forward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/3rho0.10/forward_3/forward_3_loss.txt
forward 4rho0.20
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997221
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:37
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.2lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f5d02b69b50>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 155, '_step_count': 156, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  8.94427190999916 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.656556 valid_loss:0.611416 each epoch time:0.14898
optimizer lr 0.00096  boxsize 8.94427  train_dq/boxsize 0.060938  valid_dq/boxsize 0.055747  train_dp 0.599570  valid_dp 0.602327 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:38
run time  0:00:01.170048
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth ../data/gen_by_ML/pw-auto-test/4rho0.20/forward_3/forward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_loss.txt ../data/gen_by_ML/pw-auto-test/4rho0.20/forward_3/forward_3_loss.txt
forward 5rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997269
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:39
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f4f14c0dad0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 160, '_step_count': 161, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.656931 valid_loss:0.949976 each epoch time:0.14766
optimizer lr 0.00096  boxsize 12.64911  train_dq/boxsize 0.049952  valid_dq/boxsize 0.066021  train_dp 0.507635  valid_dp 0.502570 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:40
run time  0:00:01.100915
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/5rho0.10/forward_3/forward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/5rho0.10/forward_3/forward_3_loss.txt
forward 6rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997284
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:41
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f88f5859bd0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 165, '_step_count': 166, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.509660 valid_loss:0.735908 each epoch time:0.17460
optimizer lr 0.00096  boxsize 10.69045  train_dq/boxsize 0.043310  valid_dq/boxsize 0.061219  train_dp 0.543405  valid_dp 0.554614 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:42
run time  0:00:01.120189
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/6rho0.14/forward_3/forward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/6rho0.14/forward_3/forward_3_loss.txt
forward 7rho0.27
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997299
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:43
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.27lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fe211e15fd0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 170, '_step_count': 171, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  7.69800358919501 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.722137 valid_loss:0.771950 each epoch time:0.15530
optimizer lr 0.00096  boxsize 7.69800  train_dq/boxsize 0.074882  valid_dq/boxsize 0.078940  train_dp 0.624382  valid_dp 0.634567 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:44
run time  0:00:01.122337
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth ../data/gen_by_ML/pw-auto-test/7rho0.27/forward_3/forward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_loss.txt ../data/gen_by_ML/pw-auto-test/7rho0.27/forward_3/forward_3_loss.txt
forward 8rho0.38
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997314
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:45
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.38lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.38lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.38lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fc64f7a0e90>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009603999999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 175, '_step_count': 176, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009603999999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  6.488856845230502 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.690306 valid_loss:0.850911 each epoch time:0.15127
optimizer lr 0.00094  boxsize 6.48886  train_dq/boxsize 0.074794  valid_dq/boxsize 0.089581  train_dp 0.674363  valid_dp 0.716258 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:46
run time  0:00:01.117473
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/8rho0.38/8rho0.38_5.pth ../data/gen_by_ML/pw-auto-test/8rho0.38/forward_3/forward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/8rho0.38/8rho0.38_loss.txt ../data/gen_by_ML/pw-auto-test/8rho0.38/forward_3/forward_3_loss.txt
backward 7rho0.27
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997330
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:47
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.27lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.27lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f0af9760d10>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/8rho0.38/8rho0.38_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009411919999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 180, '_step_count': 181, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009411919999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  7.69800358919501 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.683760 valid_loss:0.770081 each epoch time:0.15531
optimizer lr 0.00094  boxsize 7.69800  train_dq/boxsize 0.070615  valid_dq/boxsize 0.078934  train_dp 0.623111  valid_dp 0.633136 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:48
run time  0:00:01.119747
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth ../data/gen_by_ML/pw-auto-test/7rho0.27/backward_3/backward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_loss.txt ../data/gen_by_ML/pw-auto-test/7rho0.27/backward_3/backward_3_loss.txt
backward 6rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997345
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:49
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fb4316de090>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/7rho0.27/7rho0.27_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009411919999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 185, '_step_count': 186, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009411919999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.508945 valid_loss:0.734049 each epoch time:0.14997
optimizer lr 0.00094  boxsize 10.69045  train_dq/boxsize 0.043307  valid_dq/boxsize 0.061213  train_dp 0.542769  valid_dp 0.553010 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:50
run time  0:00:01.115317
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/6rho0.14/backward_3/backward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/6rho0.14/backward_3/backward_3_loss.txt
backward 5rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997360
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:50
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f6337410310>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/6rho0.14/6rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009411919999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 190, '_step_count': 191, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009411919999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.754773 valid_loss:0.748366 each epoch time:0.14999
optimizer lr 0.00094  boxsize 12.64911  train_dq/boxsize 0.055846  valid_dq/boxsize 0.055808  train_dp 0.505735  valid_dp 0.500036 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:51
run time  0:00:01.107629
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/5rho0.10/backward_3/backward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/5rho0.10/backward_3/backward_3_loss.txt
backward 4rho0.20
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997380
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:52
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.2lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.2lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f39b05c9e10>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/5rho0.10/5rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009411919999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 195, '_step_count': 196, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009411919999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  8.94427190999916 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(14.5641, grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 14.56409955116888
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 14.56409955116888
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 14.56409955116888
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 14.56409955116888
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 14.56409955116888
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 14.56409955116888
epoch  1  acc reg_loss 1.456409955116888
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 1.456409955116888
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 1.456409955116888
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 1.456409955116888
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 1.456409955116888
5 epoch: train_loss:0.706114 valid_loss:0.758190 each epoch time:0.14674
optimizer lr 0.00094  boxsize 8.94427  train_dq/boxsize 0.066022  valid_dq/boxsize 0.070530  train_dp 0.597830  valid_dp 0.600195 reg_loss 1.456410
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:53
run time  0:00:01.180566
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth ../data/gen_by_ML/pw-auto-test/4rho0.20/backward_3/backward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_loss.txt ../data/gen_by_ML/pw-auto-test/4rho0.20/backward_3/backward_3_loss.txt
backward 3rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997395
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:54
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f7f7a038a10>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/4rho0.20/4rho0.20_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009411919999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 200, '_step_count': 201, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009411919999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.756298 valid_loss:1.046805 each epoch time:0.14400
optimizer lr 0.00094  boxsize 12.64911  train_dq/boxsize 0.055907  valid_dq/boxsize 0.070612  train_dp 0.506165  valid_dp 0.499030 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:55
run time  0:00:01.074372
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/3rho0.10/backward_3/backward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/3rho0.10/backward_3/backward_3_loss.txt
backward 2rho0.14
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997410
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:56
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.14lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.14lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7f3bd8844290>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/3rho0.10/3rho0.10_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009411919999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 205, '_step_count': 206, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009411919999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  10.690449676496975 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.508027 valid_loss:0.590733 each epoch time:0.14871
optimizer lr 0.00094  boxsize 10.69045  train_dq/boxsize 0.043348  valid_dq/boxsize 0.050045  train_dp 0.541552  valid_dp 0.551822 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:57
run time  0:00:01.111906
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth ../data/gen_by_ML/pw-auto-test/2rho0.14/backward_3/backward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_loss.txt ../data/gen_by_ML/pw-auto-test/2rho0.14/backward_3/backward_3_loss.txt
backward 1rho0.10
optimizer initialized : op  <class 'torch.optim.sgd.SGD'>  lr  0.001
pid :  3997425
uname :  posix.uname_result(sysname='Linux', nodename='jae', release='5.11.0-25-generic', version='#27~20.04.1-Ubuntu SMP Tue Jul 13 17:41:23 UTC 2021', machine='x86_64')
code run start time  20210909, 19:50:58
my device here
pb initialized
phase_space initialized
check4particle_crash_dummy initialized
linear_integrator initialized 
MLP_net initialized :  5 -> ... -> 128 -> 2
MLP_net initialized :  5 -> ... -> 128 -> 2
hamiltonian initialized
LJ_term initialized : sigma  1  epsilon  1
lennard_jones.py call potential
pb initialized
phase_space initialized
lennard_jones initialized: sigma  1  epsilon  1
kinetic_energy initialized : mass  1
HNN_base initialized
pairwise_HNN initialized 
my_data initialized : train_filename  ../data/gen_by_MD/train/n16rho0.1lt0.1nsamples24000_shuffle.pt  val_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  test_filename  ../data/gen_by_MD/valid/n16rho0.1lt0.1nsamples4800_shuffle.pt  train_pts  100  val_pts  100  test_pts  100
kwargs  {}
created  SGD  with lr  0.001
checkpoint initialized : net list  [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)]  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.001
    lr: 0.001
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <torch.optim.lr_scheduler.StepLR object at 0x7fa854b4a8d0>
=> loading checkpoint '../data/gen_by_ML/pw-auto-test/2rho0.14/2rho0.14_5.pth'
{'net_list': [mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
), mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=5, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): Tanh()
    (4): Dropout(p=0, inplace=False)
    (5): Linear(in_features=64, out_features=128, bias=True)
    (6): Tanh()
    (7): Dropout(p=0, inplace=False)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'optimizer': {'state': {}, 'param_groups': [{'lr': 0.0009411919999999999, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]}, 'scheduler': {'step_size': 60, 'gamma': 0.98, 'base_lrs': [0.001], 'last_epoch': 210, '_step_count': 211, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0009411919999999999]}}
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained scheduler state_dict loaded...
MD_learner initialized : tau_cur  0.1  boxsize  12.649110640673518 pothrsh 159.5294604629323 Lambda 0.01 clip value 10.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  1  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  2  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  3  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  4  acc reg_loss 0.0
batch iter 0  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 0 sum reg_loss 0.0
batch iter 1  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 1 sum reg_loss 0.0
batch iter 2  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 2 sum reg_loss 0.0
batch iter 3  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 3 sum reg_loss 0.0
batch iter 4  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 4 sum reg_loss 0.0
batch iter 5  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 5 sum reg_loss 0.0
batch iter 6  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 6 sum reg_loss 0.0
batch iter 7  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 7 sum reg_loss 0.0
batch iter 8  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 8 sum reg_loss 0.0
batch iter 9  reg_loss tensor(0., grad_fn=<ReluBackward0>)
batch iter 9 sum reg_loss 0.0
epoch  5  acc reg_loss 0.0
5 epoch: train_loss:0.655136 valid_loss:1.046525 each epoch time:0.14912
optimizer lr 0.00094  boxsize 12.64911  train_dq/boxsize 0.050008  valid_dq/boxsize 0.070607  train_dp 0.504982  valid_dp 0.498879 reg_loss 0.000000
memory usage : 15.4  at e= 5
end date/time : 20210909, 19:50:59
run time  0:00:01.081166
mean mem :  15.4 , std mem :  0.0
cp ../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_5.pth ../data/gen_by_ML/pw-auto-test/1rho0.10/backward_3/backward_3_5.pth
cp ../data/gen_by_ML/pw-auto-test/1rho0.10/1rho0.10_loss.txt ../data/gen_by_ML/pw-auto-test/1rho0.10/backward_3/backward_3_loss.txt
