128 0.025 0.47 window sliding step 1 g
128 0.025 0.47 window sliding step 4 g
128 0.025 0.47 window sliding step 8 g
128 0.025 0.47 window sliding step 12 g
128 0.025 0.47 window sliding step 16 g
device singleton constructed for  cuda
pid :  20941
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:08:54
device singleton constructed for  cuda
nsteps  198 label idx 15 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  device singleton constructed for  cuda
pid :  20942
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:08:54
device singleton constructed for  cuda
nsteps  198 label idx 19 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x148596285b10>
=> loading checkpoint '../../results20230409/traj_len08nchain08tau0.1d256l2ew01repw10_dpt1800000/mbpw000097.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([1.8453, 0.2113], device='cuda:0', requires_grad=True), tensor([-0.0032,  0.0319], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 93, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}device singleton constructed for  cuda
pid :  20940
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:08:54
device singleton constructed for  cuda
nsteps  198 label idx 11 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  
tau_list [tensor([1.8453, 0.2113], device='cuda:0', requires_grad=True), tensor([-0.0032,  0.0319], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
device singleton constructed for  cuda
pid :  20943
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:08:54
device singleton constructed for  cuda
nsteps  198 label idx 23 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  device singleton constructed for  cuda
pid :  20939
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:08:54
device singleton constructed for  cuda
nsteps  198 label idx 8 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14efa4429b10>
=> loading checkpoint '../../results20230409/traj_len08nchain012tau0.1d256l2ew01repw10_dpt1800000/mbpw000013.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([0.5432, 0.1752], device='cuda:0', requires_grad=True), tensor([ 0.0230, -0.0198], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 12, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([0.5432, 0.1752], device='cuda:0', requires_grad=True), tensor([ 0.0230, -0.0198], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14f288421b10>
=> loading checkpoint '../../results20230409/traj_len08nchain04tau0.1d256l2ew01repw10_dpt1800000/mbpw000131.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([-28.8326,   0.2207], device='cuda:0', requires_grad=True), tensor([0.0005, 0.0510], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 126, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([-28.8326,   0.2207], device='cuda:0', requires_grad=True), tensor([0.0005, 0.0510], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x15273f581b10>
=> loading checkpoint '../../results20230409/traj_len08nchain016tau0.1d256l2ew01repw10_dpt1800000/mbpw000009.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([0.3238, 0.1663], device='cuda:0', requires_grad=True), tensor([ 0.0175, -0.0372], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 8, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x1521357c9b10>
=> loading checkpoint '../../results20230409/traj_len08nchain01tau0.1d256l2ew01repw10_dpt1800000/mbpw000163.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([2.8455, 0.1802], device='cuda:0', requires_grad=True), tensor([ 0.0029, -0.0066], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 159, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([0.3238, 0.1663], device='cuda:0', requires_grad=True), tensor([ 0.0175, -0.0372], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]

tau_list [tensor([2.8455, 0.1802], device='cuda:0', requires_grad=True), tensor([ 0.0029, -0.0066], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.4941e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5670e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5177e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8481e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1095e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6712e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5597e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.0540e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.3715e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.8428e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.3527e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7322e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.7951e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.9741e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.9011e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.3987e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.2751e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.8591e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.8382e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.8823e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.6651e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002106000266763936 t = 8
steps 8 [tensor([3.0672, 2.9675, 2.8670, 2.8274, 3.2846, 3.6186, 2.3962, 2.0842, 1.5498,
        2.6396], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00023201305332118074 t = 10
steps 10 [tensor([3.3205, 2.8074, 2.8758, 4.1504, 3.6796, 3.1898, 2.6198, 2.5495, 2.3368,
        2.5844], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022551167923272236 t = 10
steps 10 [tensor([3.7745, 2.8107, 3.5964, 2.8431, 3.0577, 2.9770, 2.4495, 2.4816, 2.5346,
        2.6935], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021423672102997653 t = 12
steps 12 [tensor([3.0499, 2.7156, 2.7497, 3.5731, 2.9959, 3.3714, 2.5153, 2.3454, 1.9599,
        2.5174], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020808984765751122 t = 12
steps 12 [tensor([2.7492, 2.6898, 2.9176, 2.9695, 2.5403, 3.5552, 2.6234, 2.5443, 1.9208,
        2.4746], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6409e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5751e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.2009e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5493e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.9445e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.9127e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.9187e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.5140e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5421e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.8403e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.4701e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.3158e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7486e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5820e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.2970e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1854e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7785e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.7379e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.2777e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.4439e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020344064498618565 t = 8
steps 8 [tensor([2.1496, 2.8058, 2.0527, 2.5407, 1.7867, 4.0149, 1.9899, 1.8064, 3.6294,
        3.4719], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.7641e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.3687e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002312379299114195 t = 11
steps 11 [tensor([2.9094, 4.0310, 3.0328, 3.1210, 2.3554, 2.6296, 2.5045, 2.7621, 2.8610,
        3.7446], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6564e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022268339273662617 t = 11
steps 11 [tensor([2.3386, 3.4010, 2.7066, 2.6938, 2.2631, 3.1591, 3.0731, 3.1719, 2.8069,
        3.2206], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.3476e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.4968e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 13
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002115678105635031 t = 13
steps 13 [tensor([2.4315, 3.3208, 2.6257, 3.6790, 2.2465, 2.9269, 2.8177, 2.4243, 2.3300,
        2.5908], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020555014941804305 t = 12
steps 12 [tensor([2.9012, 2.6308, 2.6902, 3.1848, 2.4167, 2.2858, 2.5197, 2.4896, 2.1667,
        3.3065], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.4811e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.4607e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.9444e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8470e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.5817e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021578096985259344 t = 7
steps 7 [tensor([2.0212, 2.5992, 2.3939, 2.4596, 3.3888, 4.0142, 3.5550, 2.3542, 2.1254,
        3.2727], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.4343e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.5411e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.3125e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.4666e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5041e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7497e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6671e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.2419e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1012e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6925e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.8320e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.6725e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.3706e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.7269e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.3801e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.1934e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00023232242842925147 t = 10
steps 10 [tensor([2.0941, 3.4806, 3.3107, 2.8332, 3.4256, 3.0002, 4.1713, 2.5170, 2.7925,
        2.7069], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002257320106607813 t = 11
steps 11 [tensor([2.3463, 2.9349, 3.2814, 2.6654, 3.8175, 2.8647, 2.9677, 2.6179, 2.7486,
        3.2625], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5382e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020237157666597226 t = 8
steps 8 [tensor([3.1891, 1.5509, 2.1162, 2.5497, 2.5288, 2.9841, 2.8317, 3.3554, 2.9123,
        2.3589], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.9013e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002181159986762494 t = 12
steps 12 [tensor([2.3871, 3.5186, 3.1041, 2.9305, 3.3784, 2.5555, 3.1984, 2.2592, 2.5282,
        2.6392], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 13
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.4698e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.4496e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022397728585041348 t = 13
steps 13 [tensor([1.9623, 3.2818, 3.1950, 3.2383, 3.7284, 2.7122, 2.9481, 2.3556, 3.1894,
        2.7000], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8906e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.1786e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.5015e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7768e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.3399e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.4129e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.4717e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.2898e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7058e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.4993e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.0914e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6346e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.5953e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1511e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.3224e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021139539350678653 t = 11
steps 11 [tensor([2.6761, 1.8236, 2.5184, 3.1222, 2.9319, 2.3424, 2.3066, 2.8492, 4.4727,
        2.5099], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.6415e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.3573e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021520362725776693 t = 8
steps 8 [tensor([2.8268, 2.6510, 2.1515, 4.0023, 2.8344, 2.5042, 2.5930, 2.0746, 2.8959,
        3.3396], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:24
run time  0:00:30.022745
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022711295481124365 t = 11
steps 11 [tensor([3.2924, 2.0584, 2.2990, 3.2264, 2.7755, 2.7777, 2.8582, 3.4086, 4.1032,
        2.7929], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6458e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.0287e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5838e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.6801e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 13
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.9706e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.6338e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020594231428974676 t = 13
steps 13 [tensor([2.4731, 1.8387, 2.1777, 2.9647, 3.2406, 2.1317, 2.3285, 2.7318, 4.3099,
        2.6752], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 13
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.5889e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.4609e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021499778502406038 t = 13
steps 13 [tensor([2.7691, 1.9411, 2.6602, 2.8423, 2.9680, 2.8355, 2.1508, 3.6433, 4.0846,
        2.1323], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5698e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8434e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021246383856137624 t = 10
steps 10 [tensor([2.6230, 3.7022, 2.6597, 3.1372, 2.4619, 2.5802, 2.8554, 2.5562, 2.5457,
        2.3618], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:29
run time  0:00:34.565706
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6133e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.2773e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7655e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.8477e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1397e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00023087513759669984 t = 11
steps 11 [tensor([2.5567, 2.9956, 2.9556, 3.9088, 3.1332, 2.9226, 3.3666, 2.4706, 2.9254,
        2.6331], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:30
run time  0:00:35.413837
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.6006e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(8.6436e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.3508e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 10
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 11
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 13
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 12
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022169175463388271 t = 13
steps 13 [tensor([2.2913, 2.8593, 3.1927, 3.9767, 2.3647, 2.9622, 3.2833, 2.4458, 2.7158,
        2.5775], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:31
run time  0:00:37.030047
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 13
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021862941363360165 t = 13
steps 13 [tensor([2.4067, 2.6637, 3.1667, 3.0947, 2.2826, 3.1184, 2.9913, 2.6017, 2.8370,
        3.1156], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.025T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:31
run time  0:00:37.133062
mean mem :  nan , std mem :  nan
128 0.25 0.44 window sliding step 1 g
128 0.25 0.44 window sliding step 4 g
128 0.25 0.44 window sliding step 8 g
128 0.25 0.44 window sliding step 12 g
128 0.25 0.44 window sliding step 16 g
device singleton constructed for  cuda
pid :  21631
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:09:33
device singleton constructed for  cuda
nsteps  198 label idx 19 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  device singleton constructed for  cuda
pid :  21629
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:09:33
device singleton constructed for  cuda
nsteps  198 label idx 11 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14c6379a9b10>
=> loading checkpoint '../../results20230409/traj_len08nchain012tau0.1d256l2ew01repw10_dpt1800000/mbpw000013.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([0.5432, 0.1752], device='cuda:0', requires_grad=True), tensor([ 0.0230, -0.0198], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 12, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([0.5432, 0.1752], device='cuda:0', requires_grad=True), tensor([ 0.0230, -0.0198], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
device singleton constructed for  cuda
pid :  21628
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:09:33
device singleton constructed for  cuda
nsteps  198 label idx 8 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  device singleton constructed for  cuda
pid :  21632
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:09:33
device singleton constructed for  cuda
nsteps  198 label idx 23 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  device singleton constructed for  cuda
pid :  21630
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:09:33
device singleton constructed for  cuda
nsteps  198 label idx 15 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14a6787edb10>
=> loading checkpoint '../../results20230409/traj_len08nchain04tau0.1d256l2ew01repw10_dpt1800000/mbpw000131.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([-28.8326,   0.2207], device='cuda:0', requires_grad=True), tensor([0.0005, 0.0510], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 126, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([-28.8326,   0.2207], device='cuda:0', requires_grad=True), tensor([0.0005, 0.0510], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14c1db0a1b10>
=> loading checkpoint '../../results20230409/traj_len08nchain01tau0.1d256l2ew01repw10_dpt1800000/mbpw000163.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([2.8455, 0.1802], device='cuda:0', requires_grad=True), tensor([ 0.0029, -0.0066], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 159, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x145532081b10>
=> loading checkpoint '../../results20230409/traj_len08nchain016tau0.1d256l2ew01repw10_dpt1800000/mbpw000009.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([0.3238, 0.1663], device='cuda:0', requires_grad=True), tensor([ 0.0175, -0.0372], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 8, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([2.8455, 0.1802], device='cuda:0', requires_grad=True), tensor([ 0.0029, -0.0066], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14a31002db10>
=> loading checkpoint '../../results20230409/traj_len08nchain08tau0.1d256l2ew01repw10_dpt1800000/mbpw000097.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([1.8453, 0.2113], device='cuda:0', requires_grad=True), tensor([-0.0032,  0.0319], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 93, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([0.3238, 0.1663], device='cuda:0', requires_grad=True), tensor([ 0.0175, -0.0372], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]

tau_list [tensor([1.8453, 0.2113], device='cuda:0', requires_grad=True), tensor([-0.0032,  0.0319], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.9208e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6472e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.7559e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.4007e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.8791e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8924e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.0792e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1779e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7050e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.8740e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.8422e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.0002820621525061552 t = 4
steps 4 [tensor([4.2370, 3.1236, 4.6665, 4.4042, 3.7186, 3.4440, 3.1909, 3.3035, 3.1611,
        3.2948], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020881286965430818 t = 5
steps 5 [tensor([2.5055, 2.3673, 3.1932, 2.9464, 3.3302, 2.5260, 2.4038, 2.5310, 2.7059,
        2.5852], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002065448610599781 t = 7
steps 7 [tensor([2.9064, 2.3947, 2.6290, 2.7895, 2.8003, 2.8294, 2.6336, 2.6294, 2.6714,
        2.5319], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002085484516709411 t = 6
steps 6 [tensor([2.3222, 2.7133, 2.8770, 2.9464, 2.7890, 2.7023, 2.6244, 2.7079, 2.5616,
        2.8323], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021030836828756432 t = 8
steps 8 [tensor([2.8311, 2.4122, 2.3002, 3.4317, 3.0305, 2.2654, 2.6420, 2.6349, 3.0607,
        2.7318], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7939e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.7878e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.8534e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.6409e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.9057e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7215e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.2976e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.2016e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.0002961035813166504 t = 4
steps 4 [tensor([3.3594, 3.8148, 4.6345, 4.6977, 4.4009, 3.3238, 2.8679, 3.4773, 3.5368,
        4.2098], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.3776e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7089e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002159217833731258 t = 5
steps 5 [tensor([2.6809, 2.6197, 3.4788, 3.1571, 2.9254, 2.6728, 2.3678, 2.8689, 2.1515,
        3.0221], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.9131e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020852367605511086 t = 6
steps 6 [tensor([2.4359, 2.6097, 3.2307, 3.1571, 2.6129, 2.4224, 2.6270, 2.7955, 2.4337,
        2.6823], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002065136222599978 t = 7
steps 7 [tensor([3.1279, 2.6256, 3.1648, 3.1161, 2.4479, 2.5061, 2.2597, 2.3962, 2.3764,
        2.6911], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6318e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.7575e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.7969e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.00028373890325448294 t = 4
steps 4 [tensor([3.4790, 4.0717, 3.9421, 2.9831, 3.4350, 4.1226, 3.7985, 3.5970, 3.9186,
        3.7142], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.5751e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002206483293054501 t = 8
steps 8 [tensor([2.8560, 2.8401, 3.1442, 3.9375, 2.7119, 2.7747, 2.4994, 2.7454, 2.0769,
        2.9815], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.8151e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.2154e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7244e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.1920e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020893609321913123 t = 5
steps 5 [tensor([2.7503, 2.6336, 2.7473, 2.8473, 2.7705, 2.9477, 2.5011, 2.4470, 2.7302,
        2.9383], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.9142e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.3106e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002067157007328689 t = 6
steps 6 [tensor([2.7600, 2.8702, 2.9575, 2.8775, 2.9904, 2.6388, 2.3993, 2.3680, 2.5673,
        2.5920], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6827e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.8078e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020374149492331784 t = 7
steps 7 [tensor([2.7372, 3.6062, 2.4950, 2.7647, 2.6033, 2.3140, 2.4505, 2.6962, 2.6617,
        2.2908], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.8741e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.00028824739209338087 t = 4
steps 4 [tensor([5.6617, 3.6920, 3.5564, 3.7027, 4.2717, 3.0434, 3.3612, 3.3904, 3.5232,
        3.4628], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.6871e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.6742e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.3103e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.9097e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020017479437940984 t = 5
steps 5 [tensor([2.7829, 2.5514, 2.9217, 2.7422, 2.9272, 2.3946, 2.6831, 2.6242, 2.1936,
        2.3104], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7540e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002072998682252867 t = 8
steps 8 [tensor([2.9732, 2.4057, 2.4089, 2.8786, 3.1282, 2.3177, 2.9200, 2.8401, 2.8396,
        2.4013], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8122e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.2647e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002169874941143025 t = 6
steps 6 [tensor([3.3169, 2.8310, 3.1332, 3.0112, 2.7970, 2.9165, 2.6938, 2.6059, 2.7605,
        2.2246], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.0002799134441186549 t = 4
steps 4 [tensor([3.7708, 3.9082, 4.4455, 4.2220, 4.0346, 2.6060, 3.0146, 3.0585, 4.0836,
        3.0781], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:54
run time  0:00:20.365998
mean mem :  nan , std mem :  nan
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.8375e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.3889e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.4780e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7259e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021134556073315657 t = 7
steps 7 [tensor([2.8013, 2.6867, 3.5720, 2.8506, 2.7189, 2.1941, 2.9292, 2.3618, 2.8632,
        2.5496], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.8191e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.7200e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.2264e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.00025919413632427205 t = 6
steps 6 [tensor([4.0831, 3.8141, 2.9656, 3.3847, 4.2183, 2.3950, 3.4578, 3.1852, 2.6552,
        3.3704], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:56
run time  0:00:22.606040
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.8818e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6429e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022313276997150183 t = 8
steps 8 [tensor([2.9709, 2.9481, 3.4608, 2.8840, 2.6124, 2.3794, 3.4574, 2.8561, 2.7084,
        2.7634], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.1299e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020732034114953631 t = 6
steps 6 [tensor([2.8900, 2.8775, 2.4494, 2.6339, 3.1295, 2.1468, 2.4881, 2.7142, 2.7543,
        2.7601], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:57
run time  0:00:23.586703
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.3148e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5073e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00024264785795711735 t = 8
steps 8 [tensor([2.9576, 2.8242, 3.1533, 3.5855, 3.5793, 2.3918, 3.0773, 3.3588, 3.0684,
        3.4196], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:58
run time  0:00:24.535183
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.5846e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(9.9684e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020160335170455398 t = 8
steps 8 [tensor([2.6166, 2.2246, 2.4842, 2.8503, 3.2453, 2.2635, 2.9235, 2.5248, 2.4529,
        2.5023], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.25T0.44/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id4.pt
end date/time every checkpoint: 20230423, 20:09:58
run time  0:00:25.002608
mean mem :  nan , std mem :  nan
128 0.66 0.47 window sliding step 1 g
128 0.66 0.47 window sliding step 4 g
128 0.66 0.47 window sliding step 8 g
128 0.66 0.47 window sliding step 12 g
128 0.66 0.47 window sliding step 16 g
device singleton constructed for  cuda
pid :  22198
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:10:00
device singleton constructed for  cuda
nsteps  198 label idx 19 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  device singleton constructed for  cuda
pid :  22196
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:10:00
device singleton constructed for  cuda
nsteps  198 label idx 11 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  device singleton constructed for  cuda
pid :  22197
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:10:00
device singleton constructed for  cuda
nsteps  198 label idx 15 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  device singleton constructed for  cuda
pid :  22195
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:10:00
device singleton constructed for  cuda
nsteps  198 label idx 8 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  device singleton constructed for  cuda
pid :  22199
uname :  uname_result(system='Linux', node='x1000c0s1b0n0', release='4.18.0-305.25.1.el8_4.x86_64', version='#1 SMP Mon Oct 18 14:34:11 EDT 2021', machine='x86_64')
code run start time  20230423, 20:10:00
device singleton constructed for  cuda
nsteps  198 label idx 23 t thrsh  198
 velocity verlet MD
 check force .....
load data init file :  torch.Size([50, 3, 8, 128, 2])
pw fnn
--- initialize pw_ff ---
mb fnn
--- initialize mb ff mlp ---
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
)
pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)
 velocity verletx 
state dict  {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 0, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}
loss initialized: rthrsh 0.7 pethrsh 288.990 e weight 1 reg weight 10 reg weight2 0.01
checkpoint initialized : net list  [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14dbe13ddb10>
=> loading checkpoint '../../results20230409/traj_len08nchain012tau0.1d256l2ew01repw10_dpt1800000/mbpw000013.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([0.5432, 0.1752], device='cuda:0', requires_grad=True), tensor([ 0.0230, -0.0198], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 12, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14ecb11ddb10>
=> loading checkpoint '../../results20230409/traj_len08nchain04tau0.1d256l2ew01repw10_dpt1800000/mbpw000131.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([-28.8326,   0.2207], device='cuda:0', requires_grad=True), tensor([0.0005, 0.0510], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 126, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([0.5432, 0.1752], device='cuda:0', requires_grad=True), tensor([ 0.0230, -0.0198], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]

tau_list [tensor([-28.8326,   0.2207], device='cuda:0', requires_grad=True), tensor([0.0005, 0.0510], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14de248cdb10>
=> loading checkpoint '../../results20230409/traj_len08nchain08tau0.1d256l2ew01repw10_dpt1800000/mbpw000097.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([1.8453, 0.2113], device='cuda:0', requires_grad=True), tensor([-0.0032,  0.0319], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 93, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([1.8453, 0.2113], device='cuda:0', requires_grad=True), tensor([-0.0032,  0.0319], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x154d7b22db10>
=> loading checkpoint '../../results20230409/traj_len08nchain016tau0.1d256l2ew01repw10_dpt1800000/mbpw000009.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([0.3238, 0.1663], device='cuda:0', requires_grad=True), tensor([ 0.0175, -0.0372], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 8, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}[pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)] tau list len 2  opt  SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)  opt2  SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) sch <optimizers.DecayCosineAnnealingWarmRestarts.DecayCosineAnnealingWarmRestarts object at 0x14d829fc9b10>
=> loading checkpoint '../../results20230409/traj_len08nchain01tau0.1d256l2ew01repw10_dpt1800000/mbpw000163.pth'
{'net_list': [pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), mb_transformer_net(
  (feat_embedder): Linear(in_features=24, out_features=256, bias=True)
  (transformer): Sequential(
    (0): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
    (1): EncoderLayer(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.5, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.5, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=256, out_features=1024, bias=True)
        (1): GELU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=1024, out_features=256, bias=True)
        (4): GELU()
        (5): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (readout): Linear(in_features=256, out_features=2, bias=True)
), pw_mlp_net(
  (layers): ModuleList(
    (0): Linear(in_features=2, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): Linear(in_features=128, out_features=2, bias=True)
  )
)], 'tau_list': [tensor([2.8455, 0.1802], device='cuda:0', requires_grad=True), tensor([ 0.0029, -0.0066], device='cuda:0', requires_grad=True)], 'optimizer': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}, 6: {'momentum_buffer': None}, 7: {'momentum_buffer': None}, 8: {'momentum_buffer': None}, 9: {'momentum_buffer': None}, 10: {'momentum_buffer': None}, 11: {'momentum_buffer': None}, 12: {'momentum_buffer': None}, 13: {'momentum_buffer': None}, 14: {'momentum_buffer': None}, 15: {'momentum_buffer': None}, 16: {'momentum_buffer': None}, 17: {'momentum_buffer': None}, 18: {'momentum_buffer': None}, 19: {'momentum_buffer': None}, 20: {'momentum_buffer': None}, 21: {'momentum_buffer': None}, 22: {'momentum_buffer': None}, 23: {'momentum_buffer': None}, 24: {'momentum_buffer': None}, 25: {'momentum_buffer': None}, 26: {'momentum_buffer': None}, 27: {'momentum_buffer': None}, 28: {'momentum_buffer': None}, 29: {'momentum_buffer': None}, 30: {'momentum_buffer': None}, 31: {'momentum_buffer': None}, 32: {'momentum_buffer': None}, 33: {'momentum_buffer': None}, 34: {'momentum_buffer': None}, 35: {'momentum_buffer': None}, 36: {'momentum_buffer': None}, 37: {'momentum_buffer': None}, 38: {'momentum_buffer': None}, 39: {'momentum_buffer': None}, 40: {'momentum_buffer': None}, 41: {'momentum_buffer': None}, 42: {'momentum_buffer': None}, 43: {'momentum_buffer': None}, 44: {'momentum_buffer': None}, 45: {'momentum_buffer': None}, 46: {'momentum_buffer': None}, 47: {'momentum_buffer': None}, 48: {'momentum_buffer': None}, 49: {'momentum_buffer': None}, 50: {'momentum_buffer': None}, 51: {'momentum_buffer': None}, 52: {'momentum_buffer': None}, 53: {'momentum_buffer': None}, 54: {'momentum_buffer': None}, 55: {'momentum_buffer': None}, 56: {'momentum_buffer': None}, 57: {'momentum_buffer': None}, 58: {'momentum_buffer': None}, 59: {'momentum_buffer': None}, 60: {'momentum_buffer': None}, 61: {'momentum_buffer': None}, 62: {'momentum_buffer': None}, 63: {'momentum_buffer': None}, 64: {'momentum_buffer': None}, 65: {'momentum_buffer': None}, 66: {'momentum_buffer': None}, 67: {'momentum_buffer': None}, 68: {'momentum_buffer': None}, 69: {'momentum_buffer': None}, 70: {'momentum_buffer': None}, 71: {'momentum_buffer': None}, 72: {'momentum_buffer': None}, 73: {'momentum_buffer': None}, 74: {'momentum_buffer': None}, 75: {'momentum_buffer': None}, 76: {'momentum_buffer': None}, 77: {'momentum_buffer': None}, 78: {'momentum_buffer': None}, 79: {'momentum_buffer': None}, 80: {'momentum_buffer': None}, 81: {'momentum_buffer': None}, 82: {'momentum_buffer': None}, 83: {'momentum_buffer': None}, 84: {'momentum_buffer': None}, 85: {'momentum_buffer': None}}, 'param_groups': [{'lr': 1e-05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'initial_lr': 1e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]}]}, 'optimizer2': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}]}, 'scheduler': {'cos_dict': {'T_0': 1, 'T_i': 1, 'T_mult': 1, 'eta_min': 0, 'T_cur': 0, 'base_lrs': [1e-05], 'last_epoch': 159, '_step_count': 0, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [1e-05]}, 'thrsh': 1e-05, 'cntr': 0}}
tau_list [tensor([0.3238, 0.1663], device='cuda:0', requires_grad=True), tensor([ 0.0175, -0.0372], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]

tau_list [tensor([2.8455, 0.1802], device='cuda:0', requires_grad=True), tensor([ 0.0029, -0.0066], device='cuda:0', requires_grad=True)]
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously net_list state_dict loaded...
Previously trained optimizer state_dict loaded...
Previously trained optimizer2 state_dict loaded...
Previously trained scheduler state_dict loaded...
 velocity verletx 
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.35 GB 

GPU memory % allocated: 0.36 GB 

sample avg init dq list tensor(7.7007e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.7694e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.1488e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.2065e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8978e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.0776e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.5995e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.3223e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.7815e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.2929e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8001e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.00029029557143287106 t = 3
steps 3 [tensor([3.5375, 3.3664, 3.6117, 4.0208, 3.7922, 4.0213, 3.4234, 4.1982, 4.3944,
        3.3579], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.2821e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.0002539114609203046 t = 5
steps 5 [tensor([4.1823, 2.6237, 3.5315, 3.4172, 2.9473, 3.6285, 3.8185, 3.2337, 2.8711,
        2.6688], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021176798901133823 t = 6
steps 6 [tensor([3.2620, 2.6314, 2.9186, 2.9266, 2.6273, 2.7545, 2.6326, 2.7311, 2.6396,
        2.3537], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022735890039487047 t = 8
steps 8 [tensor([3.1361, 2.6070, 2.8527, 3.1576, 2.6388, 2.7586, 3.2004, 2.7153, 3.3239,
        3.1427], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.0403e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.4487e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00021519684862475908 t = 9
steps 9 [tensor([2.7544, 2.6355, 2.5877, 2.9392, 3.0906, 2.6691, 2.7461, 2.6807, 3.5467,
        2.3371], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id0.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.000290271377247976 t = 3
steps 3 [tensor([4.0736, 2.9600, 3.7418, 4.7504, 3.5743, 3.6026, 3.6604, 3.9420, 3.3018,
        3.9476], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.3381e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.8252e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.2983e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.0103e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.9862e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7338e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.00025604069300430885 t = 5
steps 5 [tensor([3.8740, 3.1931, 2.5312, 2.9951, 3.0649, 3.0781, 4.5196, 3.8696, 2.6811,
        3.2912], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.4274e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7173e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1170e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.0605e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022670559484443714 t = 6
steps 6 [tensor([3.1477, 2.7986, 2.8497, 2.3808, 2.8583, 2.9232, 3.0796, 3.0534, 2.8642,
        3.3364], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.3966e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.0002942167267830637 t = 3
steps 3 [tensor([3.7353, 3.4718, 3.4474, 4.0685, 3.7775, 3.6221, 3.9882, 4.0415, 4.9399,
        3.4126], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020677789469978675 t = 7
steps 7 [tensor([2.5502, 2.9830, 2.6518, 2.2892, 2.4388, 2.4650, 4.1108, 2.8256, 2.0811,
        2.3438], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.3731e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.2169e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.1689e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.3772e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1157e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002104253943152299 t = 8
steps 8 [tensor([3.1909, 2.5624, 2.3041, 2.9200, 2.1640, 2.9485, 3.5264, 2.6095, 2.2051,
        2.7510], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id1.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.00026018908376008705 t = 5
steps 5 [tensor([3.3847, 4.1847, 2.7069, 3.7139, 2.7300, 3.3668, 3.6509, 2.9561, 4.3067,
        3.0555], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.8973e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.00027730952663260677 t = 3
steps 3 [tensor([3.8749, 3.1258, 3.8227, 3.2348, 3.6436, 3.8072, 4.0548, 3.6751, 3.3699,
        3.5235], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.0085e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.6820e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6547e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022946694695451798 t = 6
steps 6 [tensor([2.9261, 3.2522, 2.7158, 3.0269, 3.0116, 2.8290, 3.5833, 2.9212, 3.0503,
        2.6667], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6005e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8740e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.1786e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1397e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.3474e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.8839e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022283219932448686 t = 7
steps 7 [tensor([2.5689, 3.1499, 2.6660, 2.8973, 2.9858, 2.4244, 2.9029, 3.3745, 3.5474,
        2.6176], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.1802e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.00028524306300103155 t = 3
steps 3 [tensor([3.5108, 3.2532, 3.6171, 4.9301, 3.0734, 4.2549, 3.0956, 3.6536, 3.6979,
        3.8260], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C1d256l2mbpw163t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:10:18
run time  0:00:17.946210
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.3877e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.0002643098680643961 t = 5
steps 5 [tensor([3.0830, 3.0487, 4.8219, 3.3951, 3.0057, 3.8860, 2.8440, 4.0125, 2.9341,
        3.4231], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 9
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.8864e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.0002132061404209587 t = 9
steps 9 [tensor([2.9856, 3.0939, 2.4501, 2.7758, 2.7467, 2.5616, 2.8500, 3.1466, 2.9086,
        2.3374], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id2.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.1694e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

GPU memory % allocated: 0.39 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022496205701341826 t = 6
steps 6 [tensor([3.0742, 2.8677, 2.7967, 3.3522, 2.8397, 3.0012, 2.5272, 3.5015, 2.6684,
        2.7442], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.2670e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.2246e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.6703e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.6443e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.1665e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.000204041034525993 t = 7
steps 7 [tensor([2.4839, 2.7924, 3.2082, 2.4111, 2.7433, 2.7617, 2.4713, 2.9189, 2.2695,
        2.5477], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.9583e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0003, device='cuda:0')
L > eps ..... 0.0002705058253039385 t = 5
steps 5 [tensor([4.0663, 3.2451, 3.3716, 2.3867, 4.0418, 3.9992, 3.2117, 2.6863, 3.0167,
        4.9393], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C8d256l2mbpw097t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:10:22
run time  0:00:21.577376
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(8.4665e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.3095e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(6.8220e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00022617401708706337 t = 6
steps 6 [tensor([3.4318, 3.0446, 3.0108, 2.6481, 2.8056, 2.8267, 2.6992, 2.6576, 2.7221,
        3.4068], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C4d256l2mbpw131t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:10:23
run time  0:00:22.966733
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.8447e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.000212873617311944 t = 8
steps 8 [tensor([2.6166, 3.0197, 3.1709, 2.4787, 2.5333, 2.9360, 2.6419, 3.0716, 2.7448,
        2.5020], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id3.pt
sample for dt traj ==== torch.Size([10, 3, 8, 128, 2])
guess t max  20.5
t thrsh ====== [0]
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.5796e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
start t =======
q init shape torch.Size([10, 128, 2]) q dt init shape torch.Size([10, 128, 2])
GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

GPU memory % allocated: 0.4 GB 

sample avg init dq list tensor(7.7283e-05, device='cuda:0')
t accum ====== [0]
increment t until eps ======= 1
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.7031e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 2
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020221215465296237 t = 7
steps 7 [tensor([2.6607, 2.5570, 2.6044, 2.7190, 2.9677, 3.1918, 2.4003, 2.1087, 2.4366,
        2.4912], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C12d256l2mbpw013t24.7_tau0.1_lyapunovML_id4.pt
end date/time every checkpoint: 20230423, 20:10:24
run time  0:00:23.999998
mean mem :  nan , std mem :  nan
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(7.9876e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 3
GPU memory % allocated: 0.36 GB 

sample avg dq list tensor(9.4642e-05, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 4
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 5
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0001, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 6
GPU memory % allocated: 0.37 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 7
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L < eps .....
t accum ====== [0]
increment t until eps ======= 8
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/app/apps/pytorch/1.11.0-py3-gpu/lib/python3.10/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
GPU memory % allocated: 0.38 GB 

sample avg dq list tensor(0.0002, device='cuda:0')
L > eps ..... 0.00020976448661501736 t = 8
steps 8 [tensor([3.3828, 2.2768, 2.6473, 2.8484, 2.7778, 3.3521, 2.2446, 2.0956, 2.4400,
        3.0487], device='cuda:0')]
save file dir ../../../data_sets/gen_by_ML/lt0.1dpt1800000/n128rho0.66T0.47/pred_len08C16d256l2mbpw009t24.7_tau0.1_lypunovML_id4.pt
end date/time every checkpoint: 20230423, 20:10:25
run time  0:00:24.595176
mean mem :  nan , std mem :  nan
======================================================================================

			Resource Usage on 2023-04-23 20:10:26.023715:

	JobId: 1194708.pbs101  
	Project: 13003073 
	Exit Status: 0
	NCPUs Requested: 16				NCPUs Used: 16
							CPU Time Used: 00:01:07
	Memory Requested: 110gb 			Memory Used: 9454468kb
							Vmem Used: 69898700kb
	Walltime requested: 120:00:00 			Walltime Used: 00:01:34
	
	Execution Nodes Used: (x1000c0s1b0n0:ngpus=1:ncpus=16:mem=115343360kb)
	
 ======================================================================================
